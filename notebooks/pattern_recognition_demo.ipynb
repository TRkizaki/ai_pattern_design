{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a4981-d202-4da5-ae06-eab84845d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern Recognition: Master‚Äôs Course Presentation\n",
    "\n",
    "# Interactive Jupyter Notebook for Live Demo\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, silhouette_score\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings(‚Äòignore‚Äô)\n",
    "\n",
    "# Set style for better presentation\n",
    "\n",
    "plt.style.use(‚Äòseaborn-v0_8‚Äô)\n",
    "sns.set_palette(‚Äúhusl‚Äù)\n",
    "\n",
    "print(‚Äúü§ñ Pattern Recognition Interactive Demo Loaded!‚Äù)\n",
    "print(‚Äù=‚Äù * 50)\n",
    "\n",
    "# ====================================\n",
    "\n",
    "# 1. NEAREST NEIGHBOR DEMONSTRATION\n",
    "\n",
    "# ====================================\n",
    "\n",
    "class NearestNeighborDemo:\n",
    "def **init**(self):\n",
    "self.X, self.y = make_classification(\n",
    "n_samples=200, n_features=2, n_redundant=0,\n",
    "n_informative=2, n_clusters_per_class=1,\n",
    "random_state=42\n",
    ")\n",
    "self.test_points = []\n",
    "\n",
    "```\n",
    "def plot_knn_interactive(self, k=1):\n",
    "    \"\"\"Interactive K-NN visualization\"\"\"\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(self.X, self.y)\n",
    "    \n",
    "    # Create mesh for decision boundary\n",
    "    h = 0.1\n",
    "    x_min, x_max = self.X[:, 0].min() - 1, self.X[:, 0].max() + 1\n",
    "    y_min, y_max = self.X[:, 1].min() - 1, self.X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    \n",
    "    # Plot training data\n",
    "    scatter = plt.scatter(self.X[:, 0], self.X[:, 1], c=self.y, \n",
    "                        cmap='RdYlBu', edgecolors='black', s=100)\n",
    "    \n",
    "    # Plot test points if any\n",
    "    if self.test_points:\n",
    "        test_X = np.array(self.test_points)\n",
    "        test_pred = knn.predict(test_X)\n",
    "        plt.scatter(test_X[:, 0], test_X[:, 1], c=test_pred, \n",
    "                   marker='*', s=200, cmap='RdYlBu', \n",
    "                   edgecolors='black', linewidth=2)\n",
    "    \n",
    "    plt.title(f'K-Nearest Neighbor Classification (K={k})', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Feature 1', fontsize=14)\n",
    "    plt.ylabel('Feature 2', fontsize=14)\n",
    "    plt.colorbar(scatter, label='Class')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and display accuracy\n",
    "    accuracy = cross_val_score(knn, self.X, self.y, cv=5).mean()\n",
    "    print(f\"üìä Cross-validation Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"üéØ Training Samples: {len(self.X)}\")\n",
    "    print(f\"‚≠ê Test Points: {len(self.test_points)}\")\n",
    "\n",
    "def add_test_point(self, x, y):\n",
    "    \"\"\"Add a test point for classification\"\"\"\n",
    "    self.test_points.append([x, y])\n",
    "    print(f\"‚úÖ Added test point at ({x:.2f}, {y:.2f})\")\n",
    "\n",
    "def clear_test_points(self):\n",
    "    \"\"\"Clear all test points\"\"\"\n",
    "    self.test_points = []\n",
    "    print(\"üóëÔ∏è Cleared all test points\")\n",
    "```\n",
    "\n",
    "# Create demo instance\n",
    "\n",
    "nn_demo = NearestNeighborDemo()\n",
    "\n",
    "# Interactive widget for K-NN\n",
    "\n",
    "def interactive_knn(k=1):\n",
    "nn_demo.plot_knn_interactive(k)\n",
    "\n",
    "k_slider = widgets.IntSlider(value=1, min=1, max=15, step=1, description=‚ÄòK Value:‚Äô)\n",
    "widgets.interact(interactive_knn, k=k_slider)\n",
    "\n",
    "# ====================================\n",
    "\n",
    "# 2. BAYESIAN CLASSIFICATION DEMO\n",
    "\n",
    "# ====================================\n",
    "\n",
    "class BayesianDemo:\n",
    "def **init**(self):\n",
    "# Generate overlapping Gaussian data\n",
    "np.random.seed(42)\n",
    "self.class1 = np.random.multivariate_normal([2, 2], [[1, 0.5], [0.5, 1]], 100)\n",
    "self.class2 = np.random.multivariate_normal([4, 4], [[1, -0.3], [-0.3, 1]], 100)\n",
    "\n",
    "```\n",
    "    self.X = np.vstack([self.class1, self.class2])\n",
    "    self.y = np.hstack([np.zeros(100), np.ones(100)])\n",
    "\n",
    "def plot_bayesian_classification(self, prior_weight=0.5):\n",
    "    \"\"\"Visualize Bayesian classification with adjustable priors\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Create Naive Bayes classifier\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(self.X, self.y)\n",
    "    \n",
    "    # Manually adjust class priors\n",
    "    nb.class_prior_ = np.array([prior_weight, 1 - prior_weight])\n",
    "    \n",
    "    # Create mesh for decision boundary\n",
    "    h = 0.1\n",
    "    x_min, x_max = self.X[:, 0].min() - 1, self.X[:, 0].max() + 1\n",
    "    y_min, y_max = self.X[:, 1].min() - 1, self.X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Plot 1: Decision boundary\n",
    "    Z = nb.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax1.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    scatter1 = ax1.scatter(self.X[:, 0], self.X[:, 1], c=self.y, \n",
    "                          cmap='RdYlBu', edgecolors='black', s=80)\n",
    "    ax1.set_title(f'Bayesian Decision Boundary\\n(Prior: Class 0 = {prior_weight:.1f})', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Feature 1')\n",
    "    ax1.set_ylabel('Feature 2')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Probability contours\n",
    "    Z_proba = nb.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z_proba = Z_proba.reshape(xx.shape)\n",
    "    \n",
    "    contour = ax2.contourf(xx, yy, Z_proba, levels=20, cmap='RdYlBu', alpha=0.7)\n",
    "    ax2.scatter(self.X[:, 0], self.X[:, 1], c=self.y, \n",
    "               cmap='RdYlBu', edgecolors='black', s=80)\n",
    "    ax2.set_title('Posterior Probability P(Class=1|x)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Feature 1')\n",
    "    ax2.set_ylabel('Feature 2')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    plt.colorbar(contour, ax=ax2, label='P(Class=1|x)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = cross_val_score(nb, self.X, self.y, cv=5).mean()\n",
    "    log_likelihood = nb.score(self.X, self.y)\n",
    "    \n",
    "    print(f\"üìä Cross-validation Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"üìà Log Likelihood: {log_likelihood:.3f}\")\n",
    "    print(f\"üéØ Class Priors: [{prior_weight:.1f}, {1-prior_weight:.1f}]\")\n",
    "    print(f\"‚úÖ Theoretically Optimal: Yes (MAP Classifier)\")\n",
    "```\n",
    "\n",
    "# Create Bayesian demo\n",
    "\n",
    "bayes_demo = BayesianDemo()\n",
    "\n",
    "# Interactive widget for Bayesian classification\n",
    "\n",
    "def interactive_bayesian(prior_weight=0.5):\n",
    "bayes_demo.plot_bayesian_classification(prior_weight)\n",
    "\n",
    "prior_slider = widgets.FloatSlider(value=0.5, min=0.1, max=0.9, step=0.1,\n",
    "description=‚ÄòPrior Weight:‚Äô)\n",
    "widgets.interact(interactive_bayesian, prior_weight=prior_slider)\n",
    "\n",
    "# ====================================\n",
    "\n",
    "# 3. K-MEANS CLUSTERING DEMO\n",
    "\n",
    "# ====================================\n",
    "\n",
    "class KMeansDemo:\n",
    "def **init**(self):\n",
    "self.X, _ = make_blobs(n_samples=300, centers=4, n_features=2,\n",
    "random_state=42, cluster_std=1.5)\n",
    "self.animation_data = []\n",
    "\n",
    "```\n",
    "def kmeans_step_by_step(self, k=3, max_iters=10):\n",
    "    \"\"\"Visualize K-means algorithm step by step\"\"\"\n",
    "    # Initialize centroids randomly\n",
    "    centroids = self.X[np.random.choice(self.X.shape[0], k, replace=False)]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for iteration in range(min(max_iters, 6)):\n",
    "        ax = axes[iteration]\n",
    "        \n",
    "        # Assign points to closest centroid\n",
    "        distances = np.sqrt(((self.X - centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "        labels = np.argmin(distances, axis=0)\n",
    "        \n",
    "        # Plot data points\n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']\n",
    "        for i in range(k):\n",
    "            mask = labels == i\n",
    "            ax.scatter(self.X[mask, 0], self.X[mask, 1], \n",
    "                      c=colors[i], alpha=0.7, s=60)\n",
    "        \n",
    "        # Plot centroids\n",
    "        ax.scatter(centroids[:, 0], centroids[:, 1], \n",
    "                  c='black', marker='x', s=200, linewidths=3)\n",
    "        \n",
    "        ax.set_title(f'Iteration {iteration + 1}', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids = np.array([self.X[labels == i].mean(axis=0) \n",
    "                                for i in range(k)])\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.allclose(centroids, new_centroids, rtol=1e-4):\n",
    "            # Fill remaining subplots with final result\n",
    "            for j in range(iteration + 1, 6):\n",
    "                axes[j].scatter(self.X[:, 0], self.X[:, 1], c=labels, \n",
    "                               cmap='tab10', alpha=0.7, s=60)\n",
    "                axes[j].scatter(centroids[:, 0], centroids[:, 1], \n",
    "                               c='black', marker='x', s=200, linewidths=3)\n",
    "                axes[j].set_title(f'Converged (Iteration {iteration + 1})', \n",
    "                                 fontsize=14, fontweight='bold')\n",
    "                axes[j].grid(True, alpha=0.3)\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "    \n",
    "    plt.suptitle(f'K-Means Clustering Step-by-Step (K={k})', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    final_labels = kmeans.fit_predict(self.X)\n",
    "    inertia = kmeans.inertia_\n",
    "    silhouette = silhouette_score(self.X, final_labels)\n",
    "    \n",
    "    print(f\"üìä Final Inertia (WCSS): {inertia:.2f}\")\n",
    "    print(f\"üìà Silhouette Score: {silhouette:.3f}\")\n",
    "    print(f\"üéØ Number of Clusters: {k}\")\n",
    "    print(f\"‚úÖ Converged: Yes\")\n",
    "```\n",
    "\n",
    "# Create K-means demo\n",
    "\n",
    "kmeans_demo = KMeansDemo()\n",
    "\n",
    "# Interactive widget for K-means\n",
    "\n",
    "def interactive_kmeans(k=3):\n",
    "kmeans_demo.kmeans_step_by_step(k)\n",
    "\n",
    "k_clusters_slider = widgets.IntSlider(value=3, min=2, max=8, step=1,\n",
    "description=‚ÄòK Clusters:‚Äô)\n",
    "widgets.interact(interactive_kmeans, k=k_clusters_slider)\n",
    "\n",
    "# ====================================\n",
    "\n",
    "# 4. FEATURE DIMENSIONALITY DEMO\n",
    "\n",
    "# ====================================\n",
    "\n",
    "def curse_of_dimensionality_demo():\n",
    "‚Äú‚Äù‚ÄúDemonstrate the curse of dimensionality‚Äù‚Äù‚Äù\n",
    "dimensions = range(1, 21)\n",
    "sample_sizes = [50, 100, 200, 500]\n",
    "\n",
    "```\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, n_samples in enumerate(sample_sizes):\n",
    "    accuracies = []\n",
    "    \n",
    "    for n_features in dimensions:\n",
    "        # Generate high-dimensional data\n",
    "        X, y = make_classification(\n",
    "            n_samples=n_samples, n_features=n_features,\n",
    "            n_informative=min(n_features, 5), n_redundant=0,\n",
    "            n_clusters_per_class=1, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Use K-NN classifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=3)\n",
    "        scores = cross_val_score(knn, X, y, cv=3)\n",
    "        accuracies.append(scores.mean())\n",
    "    \n",
    "    # Plot results\n",
    "    axes[idx].plot(dimensions, accuracies, 'bo-', linewidth=2, markersize=6)\n",
    "    axes[idx].set_title(f'Training Size: {n_samples} samples', \n",
    "                       fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Number of Features')\n",
    "    axes[idx].set_ylabel('Cross-validation Accuracy')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim([0.4, 1.0])\n",
    "    \n",
    "    # Highlight optimal point\n",
    "    optimal_idx = np.argmax(accuracies)\n",
    "    axes[idx].scatter(dimensions[optimal_idx], accuracies[optimal_idx], \n",
    "                     color='red', s=100, zorder=5)\n",
    "    axes[idx].annotate(f'Optimal: {dimensions[optimal_idx]} features', \n",
    "                      xy=(dimensions[optimal_idx], accuracies[optimal_idx]),\n",
    "                      xytext=(10, 10), textcoords='offset points',\n",
    "                      bbox=dict(boxstyle='round,pad=0.3', fc='yellow', alpha=0.7),\n",
    "                      arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.suptitle('Curse of Dimensionality: Accuracy vs Number of Features', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Key Observations:\")\n",
    "print(\"1. With small datasets, performance degrades quickly as dimensions increase\")\n",
    "print(\"2. Larger datasets can handle more dimensions before performance drops\")\n",
    "print(\"3. There's always an optimal number of features for each dataset size\")\n",
    "print(\"4. Beyond the optimal point, adding features hurts performance\")\n",
    "```\n",
    "\n",
    "# Run the curse of dimensionality demo\n",
    "\n",
    "curse_of_dimensionality_demo()\n",
    "\n",
    "# ====================================\n",
    "\n",
    "# 5. ALGORITHM COMPARISON\n",
    "\n",
    "# ====================================\n",
    "\n",
    "def algorithm_comparison():\n",
    "‚Äú‚Äù‚ÄúCompare different classification algorithms‚Äù‚Äù‚Äù\n",
    "# Generate different types of datasets\n",
    "datasets = {\n",
    "‚ÄòLinearly Separable‚Äô: make_classification(n_samples=200, n_features=2,\n",
    "n_redundant=0, n_informative=2,\n",
    "n_clusters_per_class=1,\n",
    "class_sep=2.0, random_state=42),\n",
    "‚ÄòOverlapping Classes‚Äô: make_classification(n_samples=200, n_features=2,\n",
    "n_redundant=0, n_informative=2,\n",
    "n_clusters_per_class=1,\n",
    "class_sep=0.5, random_state=42),\n",
    "‚ÄòNon-linear Boundary‚Äô: make_classification(n_samples=200, n_features=2,\n",
    "n_redundant=0, n_informative=2,\n",
    "n_clusters_per_class=2,\n",
    "random_state=42)\n",
    "}\n",
    "\n",
    "```\n",
    "algorithms = {\n",
    "    '1-NN': KNeighborsClassifier(n_neighbors=1),\n",
    "    '5-NN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(len(datasets), len(algorithms), \n",
    "                        figsize=(18, 15))\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i, (dataset_name, (X, y)) in enumerate(datasets.items()):\n",
    "    results[dataset_name] = {}\n",
    "    \n",
    "    for j, (alg_name, algorithm) in enumerate(algorithms.items()):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        # Fit algorithm\n",
    "        algorithm.fit(X, y)\n",
    "        \n",
    "        # Create decision boundary\n",
    "        h = 0.02\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                            np.arange(y_min, y_max, h))\n",
    "        \n",
    "        Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        # Plot\n",
    "        ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "        scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', \n",
    "                           edgecolors='black', s=60)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = cross_val_score(algorithm, X, y, cv=5).mean()\n",
    "        results[dataset_name][alg_name] = accuracy\n",
    "        \n",
    "        ax.set_title(f'{alg_name}\\nAccuracy: {accuracy:.3f}', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add dataset label on the left\n",
    "        if j == 0:\n",
    "            ax.text(-0.3, 0.5, dataset_name, transform=ax.transAxes,\n",
    "                   fontsize=12, fontweight='bold', rotation=90,\n",
    "                   verticalalignment='center')\n",
    "\n",
    "plt.suptitle('Algorithm Comparison Across Different Scenarios', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\nüìä ALGORITHM PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Dataset':<20} {'1-NN':<10} {'5-NN':<10} {'Naive Bayes':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for dataset_name, scores in results.items():\n",
    "    print(f\"{dataset_name:<20} {scores['1-NN']:<10.3f} {scores['5-NN']:<10.3f} {scores['Naive Bayes']:<12.3f}\")\n",
    "\n",
    "print(\"\\nüéØ Key Insights:\")\n",
    "print(\"‚Ä¢ 1-NN is sensitive to noise but captures complex boundaries\")\n",
    "print(\"‚Ä¢ 5-NN is more robust but may oversimplify boundaries\")\n",
    "print(\"‚Ä¢ Naive Bayes assumes feature independence but is fast and stable\")\n",
    "```\n",
    "\n",
    "# Run algorithm comparison\n",
    "\n",
    "algorithm_comparison()\n",
    "\n",
    "# ====================================\n",
    "\n",
    "# 6. REAL-WORLD CASE STUDY: FISH CLASSIFICATION\n",
    "\n",
    "# ====================================\n",
    "\n",
    "def fish_classification_demo():\n",
    "‚Äú‚Äù‚ÄúDemonstrate fish classification with multiple features‚Äù‚Äù‚Äù\n",
    "np.random.seed(42)\n",
    "\n",
    "```\n",
    "# Generate synthetic fish data\n",
    "n_samples = 150\n",
    "\n",
    "# Salmon data (generally shorter, darker)\n",
    "salmon_length = np.random.normal(25, 4, n_samples//2)  # cm\n",
    "salmon_lightness = np.random.normal(30, 8, n_samples//2)  # lightness scale 0-100\n",
    "salmon_weight = 0.8 * salmon_length + np.random.normal(0, 2, n_samples//2)\n",
    "salmon_width = 0.3 * salmon_length + np.random.normal(0, 1, n_samples//2)\n",
    "\n",
    "# Bass data (generally longer, lighter)\n",
    "bass_length = np.random.normal(35, 5, n_samples//2)  # cm\n",
    "bass_lightness = np.random.normal(60, 10, n_samples//2)  # lightness scale 0-100\n",
    "bass_weight = 0.9 * bass_length + np.random.normal(0, 3, n_samples//2)\n",
    "bass_width = 0.35 * bass_length + np.random.normal(0, 1.5, n_samples//2)\n",
    "\n",
    "# Combine data\n",
    "features = {\n",
    "    'Length (cm)': np.concatenate([salmon_length, bass_length]),\n",
    "    'Lightness': np.concatenate([salmon_lightness, bass_lightness]),\n",
    "    'Weight (g)': np.concatenate([salmon_weight, bass_weight]),\n",
    "    'Width (cm)': np.concatenate([salmon_width, bass_width])\n",
    "}\n",
    "\n",
    "labels = np.concatenate([np.zeros(n_samples//2), np.ones(n_samples//2)])\n",
    "fish_names = ['Salmon', 'Bass']\n",
    "\n",
    "# Test with different feature combinations\n",
    "feature_combinations = [\n",
    "    ['Length (cm)'],\n",
    "    ['Length (cm)', 'Lightness'],\n",
    "    ['Length (cm)', 'Lightness', 'Weight (g)'],\n",
    "    ['Length (cm)', 'Lightness', 'Weight (g)', 'Width (cm)']\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, feature_combo in enumerate(feature_combinations):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Prepare data\n",
    "    X = np.column_stack([features[feat] for feat in feature_combo])\n",
    "    \n",
    "    # Train classifiers\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    nb = GaussianNB()\n",
    "    \n",
    "    knn_scores = cross_val_score(knn, X, labels, cv=5)\n",
    "    nb_scores = cross_val_score(nb, X, labels, cv=5)\n",
    "    \n",
    "    knn_acc = knn_scores.mean()\n",
    "    nb_acc = nb_scores.mean()\n",
    "    \n",
    "    results.append({\n",
    "        'features': len(feature_combo),\n",
    "        'feature_names': ', '.join(feature_combo),\n",
    "        'knn_accuracy': knn_acc,\n",
    "        'nb_accuracy': nb_acc\n",
    "    })\n",
    "    \n",
    "    # Plot performance comparison\n",
    "    methods = ['5-NN', 'Naive Bayes']\n",
    "    accuracies = [knn_acc, nb_acc]\n",
    "    colors = ['skyblue', 'lightcoral']\n",
    "    \n",
    "    bars = ax.bar(methods, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylim([0.5, 1.0])\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'{len(feature_combo)} Feature(s): {\", \".join(feature_combo[:2])}{\"...\" if len(feature_combo) > 2 else \"\"}', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add accuracy labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "               f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Fish Classification: Impact of Feature Selection', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create feature importance visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Accuracy vs Number of Features\n",
    "feature_counts = [r['features'] for r in results]\n",
    "knn_accs = [r['knn_accuracy'] for r in results]\n",
    "nb_accs = [r['nb_accuracy'] for r in results]\n",
    "\n",
    "ax1.plot(feature_counts, knn_accs, 'o-', label='5-NN', linewidth=2, markersize=8)\n",
    "ax1.plot(feature_counts, nb_accs, 's-', label='Naive Bayes', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Features')\n",
    "ax1.set_ylabel('Cross-validation Accuracy')\n",
    "ax1.set_title('Classification Performance vs Feature Count', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0.6, 1.0])\n",
    "\n",
    "# Plot 2: Feature distribution visualization (Length vs Lightness)\n",
    "ax2.scatter(features['Length (cm)'][:n_samples//2], \n",
    "           features['Lightness'][:n_samples//2], \n",
    "           c='red', alpha=0.6, label='Salmon', s=60)\n",
    "ax2.scatter(features['Length (cm)'][n_samples//2:], \n",
    "           features['Lightness'][n_samples//2:], \n",
    "           c='blue', alpha=0.6, label='Bass', s=60)\n",
    "ax2.set_xlabel('Length (cm)')\n",
    "ax2.set_ylabel('Lightness')\n",
    "ax2.set_title('Fish Features Distribution', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\nüêü FISH CLASSIFICATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Features':<10} {'Feature Names':<30} {'5-NN Acc':<10} {'NB Acc':<10}\")\n",
    "print(\"-\" * 70)\n",
    "for r in results:\n",
    "    print(f\"{r['features']:<10} {r['feature_names'][:28]:<30} {r['knn_accuracy']:<10.3f} {r['nb_accuracy']:<10.3f}\")\n",
    "\n",
    "print(\"\\nüéØ Key Findings:\")\n",
    "print(\"‚Ä¢ Single feature (length) gives ~75% accuracy\")\n",
    "print(\"‚Ä¢ Adding lightness improves performance significantly\")\n",
    "print(\"‚Ä¢ Additional features provide marginal improvements\")\n",
    "print(\"‚Ä¢ Naive Bayes performs consistently well across feature sets\")\n",
    "print(\"‚Ä¢ 5-NN benefits more from additional relevant features\")\n",
    "```\n",
    "\n",
    "# Run fish classification demo\n",
    "\n",
    "fish_classification_demo()\n",
    "\n",
    "# ====================================\n",
    "\n",
    "# 7. ADVANCED CLUSTERING DEMO\n",
    "\n",
    "# ====================================\n",
    "\n",
    "def advanced_clustering_demo():\n",
    "‚Äú‚Äù‚ÄúCompare different clustering algorithms‚Äù‚Äù‚Äù\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "\n",
    "```\n",
    "# Generate different datasets\n",
    "datasets = {\n",
    "    'Blobs': make_blobs(n_samples=300, centers=4, n_features=2, \n",
    "                       random_state=42, cluster_std=1.0)[0],\n",
    "    'Moons': make_moons(n_samples=300, noise=0.1, random_state=42)[0],\n",
    "    'Circles': make_circles(n_samples=300, noise=0.05, factor=0.6, random_state=42)[0]\n",
    "}\n",
    "\n",
    "algorithms = {\n",
    "    'K-Means': KMeans(n_clusters=3, random_state=42),\n",
    "    'DBSCAN': DBSCAN(eps=0.3, min_samples=5),\n",
    "    'Hierarchical': AgglomerativeClustering(n_clusters=3)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(len(datasets), len(algorithms), \n",
    "                        figsize=(18, 15))\n",
    "\n",
    "for i, (dataset_name, X) in enumerate(datasets.items()):\n",
    "    for j, (alg_name, algorithm) in enumerate(algorithms.items()):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        # Fit algorithm\n",
    "        if alg_name == 'K-Means':\n",
    "            labels = algorithm.fit_predict(X)\n",
    "        else:\n",
    "            labels = algorithm.fit_predict(X)\n",
    "        \n",
    "        # Plot results\n",
    "        unique_labels = set(labels)\n",
    "        colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "        \n",
    "        for k, col in zip(unique_labels, colors):\n",
    "            if k == -1:  # Noise points for DBSCAN\n",
    "                col = 'black'\n",
    "                marker = 'x'\n",
    "                size = 50\n",
    "            else:\n",
    "                marker = 'o'\n",
    "                size = 60\n",
    "            \n",
    "            class_member_mask = (labels == k)\n",
    "            xy = X[class_member_mask]\n",
    "            ax.scatter(xy[:, 0], xy[:, 1], c=[col], marker=marker, s=size, alpha=0.7)\n",
    "        \n",
    "        # Calculate silhouette score (if possible)\n",
    "        if len(set(labels)) > 1 and -1 not in labels:\n",
    "            silhouette = silhouette_score(X, labels)\n",
    "            title = f'{alg_name}\\nSilhouette: {silhouette:.3f}'\n",
    "        else:\n",
    "            title = f'{alg_name}'\n",
    "        \n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add dataset label on the left\n",
    "        if j == 0:\n",
    "            ax.text(-0.2, 0.5, dataset_name, transform=ax.transAxes,\n",
    "                   fontsize=12, fontweight='bold', rotation=90,\n",
    "                   verticalalignment='center')\n",
    "\n",
    "plt.suptitle('Clustering Algorithm Comparison', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüé® CLUSTERING INSIGHTS:\")\n",
    "print(\"‚Ä¢ K-Means works best with spherical, well-separated clusters\")\n",
    "print(\"‚Ä¢ DBSCAN can find arbitrarily shaped clusters and handle noise\")\n",
    "print(\"‚Ä¢ Hierarchical clustering provides nested cluster structure\")\n",
    "print(\"‚Ä¢ Algorithm choice depends on data characteristics and domain knowledge\")\n",
    "```\n",
    "\n",
    "# Run advanced clustering demo\n",
    "\n",
    "advanced_clustering_demo()\n",
    "\n",
    "# ====================================\n",
    "\n",
    "# 8. PRESENTATION SUMMARY\n",
    "\n",
    "# ====================================\n",
    "\n",
    "def create_summary_visualization():\n",
    "‚Äú‚Äù‚ÄúCreate a comprehensive summary of all techniques‚Äù‚Äù‚Äù\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "```\n",
    "# 1. Algorithm Performance Comparison\n",
    "algorithms = ['1-NN', '5-NN', 'Naive Bayes', 'K-Means']\n",
    "speed = [3, 3, 5, 4]  # Relative speed (1-5 scale)\n",
    "accuracy = [4, 4, 4, 3]  # Relative accuracy (1-5 scale)\n",
    "interpretability = [5, 5, 4, 3]  # How easy to interpret (1-5 scale)\n",
    "\n",
    "x = np.arange(len(algorithms))\n",
    "width = 0.25\n",
    "\n",
    "ax1.bar(x - width, speed, width, label='Speed', alpha=0.8)\n",
    "ax1.bar(x, accuracy, width, label='Accuracy', alpha=0.8)\n",
    "ax1.bar(x + width, interpretability, width, label='Interpretability', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Algorithms')\n",
    "ax1.set_ylabel('Score (1-5)')\n",
    "ax1.set_title('Algorithm Characteristics Comparison', fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(algorithms)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Feature Dimensionality Impact\n",
    "dimensions = np.arange(1, 11)\n",
    "performance = [0.7, 0.85, 0.92, 0.95, 0.94, 0.91, 0.87, 0.82, 0.78, 0.73]\n",
    "\n",
    "ax2.plot(dimensions, performance, 'bo-', linewidth=3, markersize=8)\n",
    "ax2.axvline(x=4, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "ax2.text(4.2, 0.8, 'Optimal\\nFeatures', fontsize=10, fontweight='bold', \n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "ax2.set_xlabel('Number of Features')\n",
    "ax2.set_ylabel('Classification Accuracy')\n",
    "ax2.set_title('Curse of Dimensionality', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Data Size Requirements\n",
    "data_sizes = [50, 100, 200, 500, 1000]\n",
    "nn_performance = [0.65, 0.75, 0.85, 0.90, 0.92]\n",
    "bayes_performance = [0.70, 0.80, 0.88, 0.92, 0.94]\n",
    "\n",
    "ax3.plot(data_sizes, nn_performance, 'o-', label='Nearest Neighbor', linewidth=2)\n",
    "ax3.plot(data_sizes, bayes_performance, 's-', label='Bayesian', linewidth=2)\n",
    "ax3.set_xlabel('Training Set Size')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_title('Performance vs Training Data Size', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Algorithm Decision Tree\n",
    "ax4.text(0.5, 0.9, 'Algorithm Selection Guide', \n",
    "         transform=ax4.transAxes, fontsize=16, fontweight='bold', \n",
    "         ha='center')\n",
    "\n",
    "decision_text = \"\"\"\n",
    "üìä WHEN TO USE EACH ALGORITHM:\n",
    "\n",
    "üéØ Nearest Neighbor:\n",
    "‚Ä¢ Large dataset available\n",
    "‚Ä¢ Complex decision boundaries\n",
    "‚Ä¢ No assumptions about data distribution\n",
    "\n",
    "üìà Bayesian Classification:\n",
    "‚Ä¢ Prior knowledge available\n",
    "‚Ä¢ Optimal performance needed\n",
    "‚Ä¢ Features roughly independent\n",
    "\n",
    "üé® K-Means Clustering:\n",
    "‚Ä¢ Unsupervised learning\n",
    "‚Ä¢ Spherical clusters expected\n",
    "‚Ä¢ Number of clusters known\n",
    "\n",
    "üîç Feature Selection:\n",
    "‚Ä¢ Start with domain knowledge\n",
    "‚Ä¢ Use cross-validation\n",
    "‚Ä¢ Balance complexity vs performance\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.05, 0.8, decision_text, transform=ax4.transAxes, \n",
    "         fontsize=11, verticalalignment='top', fontfamily='monospace')\n",
    "ax4.axis('off')\n",
    "\n",
    "plt.suptitle('Pattern Recognition: Complete Summary', \n",
    "             fontsize=18, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "# Create final summary\n",
    "\n",
    "create_summary_visualization()\n",
    "\n",
    "print(‚Äù\\n‚Äù + ‚Äú=‚Äù*60)\n",
    "print(‚Äúüéì PATTERN RECOGNITION MASTER‚ÄôS PRESENTATION COMPLETE!‚Äù)\n",
    "print(‚Äù=‚Äù*60)\n",
    "print(‚Äúüìö Topics Covered:‚Äù)\n",
    "print(‚Äù  ‚Ä¢ Nearest Neighbor Classification‚Äù)\n",
    "print(‚Äù  ‚Ä¢ Bayesian Decision Theory‚Äù)\n",
    "print(‚Äù  ‚Ä¢ K-Means Clustering‚Äù)\n",
    "print(‚Äù  ‚Ä¢ Feature Selection & Dimensionality‚Äù)\n",
    "print(‚Äù  ‚Ä¢ Algorithm Comparison‚Äù)\n",
    "print(‚Äù  ‚Ä¢ Real-world Applications‚Äù)\n",
    "print(‚Äù\\nüéØ Ready for your presentation!‚Äù)\n",
    "print(‚Äúüí° Use these interactive demos to engage your audience!‚Äù)\n",
    "print(‚ÄúüèÜ Good luck with your master‚Äôs course!‚Äù)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
